
<br>Кирилл, привет!
<br>Спасибо за замечания!


## geo_zone_mart.py

> Рекомендую приводить координаты к радианам, чтобы расчёт был более верным. Это можно сделать с помощью стандартной функции radians (https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.radians.html)

<br>Да, я везде использую радианы, но перевожу по формуле - координаты в градусах * число Пи / 180.
<br>Пример: lat*{math.pi}/180 as lat_r
<br>Буду знать, что в PySpark есть функция для этого.


## user_mart.py

> Обрати внимание, что не для всех городов из geo.csv есть стандартная таймзона, из-за этого код здесь будет падать. Рекомендую вручную проставить для таких городов их таймзону или приводить эти города к ближайшим, у которых есть таймзона (но это нехорошее решение, потому что стоящие рядом города не всегда находятся в одной таймзоне). Или ты можешь реализовать своё решение.

<br>Да, я как раз реализовал вариант с поиском ближайших городов. Описал это в комментариях функции `load_geo_timezone`, чтобы было понятнее.


> В результате получится по одному элементу массива на каждый день, и если пользователь прожил 50 лет в Норильске, то у него здесь будут тысячи "Норильсков" :) А должен быть один, ведь он прожил там всё это время подряд.
Предлагаю такой подход. С помощью оконных функций (например, lag) определять, отличается ли текущий город в этой строке от города в предыдущей (маркер смены города). Потом оставить только строки с этим маркером, таким образом получив последовательно города пребывания.
Количество городов (travel_count) должно коррелировать с массивом городов, поэтому его можно вычислить просто как длину массива или посчитать число строк с маркером смены.

Да, действительно. Исправил.


